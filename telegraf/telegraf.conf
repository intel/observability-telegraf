# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "5s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "5s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Log at debug level.
  debug = false
  ## Log only error level messages.
  quiet = false

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
  # logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
  logfile = ""

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0d"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  # logfile_rotation_max_archives = 5

  ## Pick a timezone to use when logging or type 'local' for local time.
  ## Example: America/Chicago
  # log_with_timezone = ""

  ## Override default hostname, if empty use os.Hostname()
  hostname = "$HOSTNAME"
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false

###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################


# Send telegraf metrics to file(s)
[[outputs.file]]
  ## Files to write to, "stdout" is a specially handled file.
  files = ["stdout", "/tmp/metrics.out"]

  ## Use batch serialization format instead of line based delimiting.  The
  ## batch format allows for the production of non line based output formats and
  ## may more efficiently encode metric groups.
  # use_batch_format = false

  ## The file will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.
  # rotation_interval = "0d"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  # rotation_max_archives = 5

  ## Data format to output.
  ## Each data format has its own unique set of configuration options, read
  ## more about them here:
  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
  data_format = "influx"


# Configuration for the Prometheus client to spawn
[[outputs.prometheus_client]]
  ## Address to listen on
  listen = ":9273"

  ## Metric version controls the mapping from Telegraf metrics into
  ## Prometheus format.  When using the prometheus input, use the same value in
  ## both plugins to ensure metrics are round-tripped without modification.
  ##
  ##   example: metric_version = 1;
  ##            metric_version = 2; recommended version
  metric_version = 2

  ## Use HTTP Basic Authentication.
  # basic_username = "Foo"
  # basic_password = "Bar"

  ## If set, the IP Ranges which are allowed to access metrics.
  ##   ex: ip_range = ["192.168.0.0/24", "192.168.1.0/30"]
  # ip_range = []

  ## Path to publish the metrics on.
  # path = "/metrics"

  ## Expiration interval for each metric. 0 == no expiration
  # expiration_interval = "60s"

  ## Collectors to enable, valid entries are "gocollector" and "process".
  ## If unset, both are enabled.
  # collectors_exclude = ["gocollector", "process"]

  ## Send string metrics as Prometheus labels.
  ## Unless set to false all string metrics will be sent as labels.
  # string_as_label = true

  ## If set, enable TLS with the given certificate.
  # tls_cert = "/etc/ssl/telegraf.crt"
  # tls_key = "/etc/ssl/telegraf.key"

  ## Set one or more allowed client CA certificate file names to
  ## enable mutually authenticated TLS connections
  # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]

  ## Export metric collection time.
  export_timestamp = true


###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################


# Read specific statistics per cgroup
[[inputs.cgroup]]
  ## Directories in which to look for files, globs are supported.
  ## Consider restricting paths to the set of cgroups you really
  ## want to monitor if you have a large number of cgroups, to avoid
  ## any cardinality issues.
  paths = [
    "/sys/fs/cgroup/cpu",              # root cgroup
    "/sys/fs/cgroup/cpu/*",            # all container cgroups
    "/sys/fs/cgroup/cpu/*/*",          # all children cgroups under each container cgroup
  ]
  ## cgroup stat fields, as file names, globs are supported.
  ## these file names are appended to each path from above.
  files = ["cpuacct.usage", "cpu.cfs_period_us", "cpu.cfs_quota_us", "cpu.shares", "cpu.stat"]


# Read metrics about cpu usage
[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states
  report_active = false


# Read metrics about disk usage by mount point
[[inputs.disk]]
  ## By default stats will be gathered for all mount points.
  ## Set mount_points will restrict the stats to only the specified mount points.
  # mount_points = ["/"]

  ## Ignore mount points by filesystem type.
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs"]


# Read metrics about disk IO by device
[[inputs.diskio]]
  ## By default, telegraf will gather stats for all devices including
  ## disk partitions.
  ## Setting devices will restrict the stats to the specified devices.
  # devices = ["sda", "sdb", "vd*"]
  ## Uncomment the following line if you need disk serial numbers.
  # skip_serial_number = false
  #
  ## On systems which support it, device metadata can be added in the form of
  ## tags.
  ## Currently only Linux is supported via udev properties. You can view
  ## available properties for a device by running:
  ## 'udevadm info -q property -n /dev/sda'
  ## Note: Most, but not all, udev properties can be accessed this way. Properties
  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.
  device_tags = ["ID_FS_TYPE", "ID_FS_USAGE"]
  #
  ## Using the same metadata source as device_tags, you can also customize the
  ## name of the device via templates.
  ## The 'name_templates' parameter is a list of templates to try and apply to
  ## the device. The template may contain variables in the form of '$PROPERTY' or
  ## '${PROPERTY}'. The first template which does not contain any variables not
  ## present for the device is used as the device name tag.
  ## The typical use case is for LVM volumes, to get the VG/LV name instead of
  ## the near-meaningless DM-0 name.
  # name_templates = ["$ID_FS_LABEL","$DM_VG_NAME/$DM_LV_NAME"]


# Query given DNS server and gives statistics
[[inputs.dns_query]]
  ## servers to query
  servers = ["8.8.8.8"]

  ## Network is the network protocol name.
  network = "tcp"

  ## Domains or subdomains to query.
  domains = ["google.com"]

  ## Query record type.
  ## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.
  # record_type = "A"

  ## Dns server port.
  # port = 53

  ## Query timeout in seconds.
  # timeout = 2


# # Reads metrics from DPDK applications using v2 telemetry interface.
# [[inputs.dpdk]]
#   ## Path to DPDK telemetry socket. This shall point to v2 version of DPDK telemetry interface.
#   # socket_path = "/var/run/dpdk/rte/dpdk_telemetry.v2"
#
#   ## Duration that defines how long the connected socket client will wait for a response before terminating connection.
#   ## This includes both writing to and reading from socket. Since it's local socket access
#   ## to a fast packet processing application, the timeout should be sufficient for most users.
#   ## Setting the value to 0 disables the timeout (not recommended)
#   # socket_access_timeout = "200ms"
#
#   ## Enables telemetry data collection for selected device types.
#   ## Adding "ethdev" enables collection of telemetry from DPDK NICs (stats, xstats, link_status).
#   ## Adding "rawdev" enables collection of telemetry from DPDK Raw Devices (xstats).
#   device_types = ["ethdev"]
#
#   ## List of custom, application-specific telemetry commands to query
#   ## The list of available commands depend on the application deployed. Applications can register their own commands
#   ##   via telemetry library API http://doc.dpdk.org/guides/prog_guide/telemetry_lib.html#registering-commands
#   ## For e.g. L3 Forwarding with Power Management Sample Application this could be:
#   ##   additional_commands = ["/l3fwd-power/stats"]
#   # additional_commands = []
#
#   ## Allows turning off collecting data for individual "ethdev" commands.
#   ## Remove "/ethdev/link_status" from list to start getting link status metrics.
#   [inputs.dpdk.ethdev]
#     exclude_commands = ["/ethdev/link_status"]
#
#   ## When running multiple instances of the plugin it's recommended to add a unique tag to each instance to identify
#   ## metrics exposed by an instance of DPDK application. This is useful when multiple DPDK apps run on a single host.
#   ##  [inputs.dpdk.tags]
#   ##    dpdk_instance = "my-fwd-app"


# Returns ethtool statistics for given interfaces
[[inputs.ethtool]]
  ## List of interfaces to pull metrics for
  # interface_include = ["eth0"]

  ## List of interfaces to ignore when pulling metrics.
  # interface_exclude = ["eth1"]

  ## Some drivers declare statistics with extra whitespace, different spacing,
  ## and mix cases. This list, when enabled, can be used to clean the keys.
  ## Here are the current possible normalizations:
  ##  * snakecase: converts fooBarBaz to foo_bar_baz
  ##  * trim: removes leading and trailing whitespace
  ##  * lower: changes all capitalized letters to lowercase
  ##  * underscore: replaces spaces with underscores
  # normalize_keys = ["snakecase", "trim", "lower", "underscore"]


# # Intel PowerStat plugin enables monitoring of platform metrics (power, TDP)
# # and per-CPU metrics like temperature, power and utilization.
# [[inputs.intel_powerstat]]
#   ## The user can choose which package metrics are monitored by the plugin with
#   ## the package_metrics setting:
#   ## - The default, will collect "current_power_consumption",
#   ##   "current_dram_power_consumption" and "thermal_design_power"
#   ## - Leaving this setting empty means no package metrics will be collected
#   ## - Finally, a user can specify individual metrics to capture from the
#   ##   supported options list
#   ## Supported options:
#   ##   "current_power_consumption", "current_dram_power_consumption",
#   ##   "thermal_design_power", "max_turbo_frequency", "uncore_frequency"
#   package_metrics = ["current_power_consumption", "current_dram_power_consumption", "thermal_design_power", "max_turbo_frequency", "uncore_frequency"]
# 
#   ## The user can choose which per-CPU metrics are monitored by the plugin in
#   ## cpu_metrics array.
#   ## Empty or missing array means no per-CPU specific metrics will be collected
#   ## by the plugin.
#   ## Supported options:
#   ##   "cpu_frequency", "cpu_c0_state_residency", "cpu_c1_state_residency",
#   ##   "cpu_c6_state_residency", "cpu_busy_cycles", "cpu_temperature",
#   ##   "cpu_busy_frequency"
#   ## ATTENTION: cpu_busy_cycles is DEPRECATED - use cpu_c0_state_residency
#   cpu_metrics = ["cpu_frequency", "cpu_c0_state_residency", "cpu_c1_state_residency", "cpu_c6_state_residency", "cpu_temperature", "cpu_busy_frequency"] 


# # Read metrics from the bare metal servers via IPMI
# [[inputs.ipmi_sensor]]
#   ## optionally specify the path to the ipmitool executable
#   # path = "/usr/bin/ipmitool"
#   ##
#   ## Setting 'use_sudo' to true will make use of sudo to run ipmitool.
#   ## Sudo must be configured to allow the telegraf user to run ipmitool
#   ## without a password.
#   use_sudo = true
#   ##
#   ## optionally force session privilege level. Can be CALLBACK, USER, OPERATOR, ADMINISTRATOR
#   # privilege = "ADMINISTRATOR"
#   ##
#   ## optionally specify one or more servers via a url matching
#   ##  [username[:password]@][protocol[(address)]]
#   ##  e.g.
#   ##    root:passwd@lan(127.0.0.1)
#   ##
#   ## if no servers are specified, local machine sensor stats will be queried
#   ##
#   # servers = ["USERID:PASSW0RD@lan(192.168.1.1)"]
#
#   ## Recommended: use metric 'interval' that is a multiple of 'timeout' to avoid
#   ## gaps or overlap in pulled data
#   interval = "30s"
#
#   ## Timeout for the ipmitool command to complete
#   timeout = "20s"
#
#   ## Schema Version: (Optional, defaults to version 1)
#   metric_version = 2
#
#   ## Optionally provide the hex key for the IMPI connection.
#   # hex_key = ""
#
#   ## If ipmitool should use a cache
#   ## for me ipmitool runs about 2 to 10 times faster with cache enabled on HP G10 servers (when using ubuntu20.04)
#   ## the cache file may not work well for you if some sensors come up late
#   # use_cache = false
#
#   ## Path to the ipmitools cache file (defaults to OS temp dir)
#   ## The provided path must exist and must be writable
#   # cache_path = ""


# Gather packets and bytes throughput from iptables
[[inputs.iptables]]
  ## iptables require root access on most systems.
  ## Setting 'use_sudo' to true will make use of sudo to run iptables.
  ## Users must configure sudo to allow telegraf user to run iptables with no password.
  ## iptables can be restricted to only list command "iptables -nvL".
  use_sudo = true
  ## Setting 'use_lock' to true runs iptables with the "-w" option.
  ## Adjust your sudo settings appropriately if using this option ("iptables -w 5 -nvl")
  use_lock = false
  ## Define an alternate executable, such as "ip6tables". Default is "iptables".
  # binary = "ip6tables"
  ## defines the table to monitor:
  table = "filter"
  ## defines the chains to monitor.
  ## NOTE: iptables rules without a comment will not be monitored.
  ## Read the plugin documentation for more information.
  chains = [ "INPUT" ]

# Gathers huge pages measurements.
[[inputs.hugepages]]
  ## Supported huge page types:
  ##   - "root"     - based on root huge page control directory:
  ##                  /sys/kernel/mm/hugepages
  ##   - "per_node" - based on per NUMA node directories:
  ##                  /sys/devices/system/node/node[0-9]*/hugepages
  ##   - "meminfo"  - based on /proc/meminfo file
  # types = ["root", "per_node"]

# Get kernel statistics from /proc/vmstat
[[inputs.kernel_vmstat]]
  # no configuration


# Read metrics about memory usage
[[inputs.mem]]
  # no configuration


# Read metrics about network interface usage
[[inputs.net]]
  ## By default, telegraf gathers stats from any up interface (excluding loopback)
  ## Setting interfaces will tell it to gather these explicit interfaces,
  ## regardless of status. When specifying an interface, glob-style
  ## patterns are also supported. Eg: interfaces = ["eth*", "enp0s[0-1]", "lo"]
  ##
  # interfaces = ["eth0"]
  ##
  ## On linux systems telegraf also collects protocol stats.
  ## Setting ignore_protocol_stats to true will skip reporting of protocol metrics.
  ##
  # ignore_protocol_stats = false
  ##


# Ping given url(s) and return statistics
[[inputs.ping]]
  ## Hosts to send ping packets to.
  urls = ["google.com"]

  ## Method used for sending pings, can be either "exec" or "native".  When set
  ## to "exec" the systems ping command will be executed.  When set to "native"
  ## the plugin will send pings directly.
  ##
  ## While the default is "exec" for backwards compatibility, new deployments
  ## are encouraged to use the "native" method for improved compatibility and
  ## performance.
  # method = "exec"

  ## Number of ping packets to send per interval.  Corresponds to the "-c"
  ## option of the ping command.
  # count = 1

  ## Time to wait between sending ping packets in seconds.  Operates like the
  ## "-i" option of the ping command.
  # ping_interval = 1.0

  ## If set, the time to wait for a ping response in seconds.  Operates like
  ## the "-W" option of the ping command.
  # timeout = 1.0

  ## If set, the total ping deadline, in seconds.  Operates like the -w option
  ## of the ping command.
  # deadline = 10

  ## Interface or source address to send ping from.  Operates like the -I or -S
  ## option of the ping command.
  # interface = ""

  ## Percentiles to calculate. This only works with the native method.
  # percentiles = [50, 95, 99]

  ## Specify the ping executable binary.
  # binary = "ping"

  ## Arguments for ping command. When arguments is not empty, the command from
  ## the binary option will be used and other options (ping_interval, timeout,
  ## etc) will be ignored.
  # arguments = ["-c", "3"]

  ## Use only IPv6 addresses when resolving a hostname.
  # ipv6 = false

  ## Number of data bytes to be sent. Corresponds to the "-s"
  ## option of the ping command. This only works with the native method.
  # size = 56


# # Read CPU, Fans, Powersupply and Voltage metrics of hardware server through redfish APIs
# [[inputs.redfish]]
#   ## For quick check proper work of redfish plugin, you can do a mockup:
#   ## Mockup must be preformed on HOST!
#   ## 1. Get a source code: git clone https://opendev.org/x/python-redfish.git
#   ## 2. Go into dmtf/mockup_0.99.0a folder.
#   ## 3. Run ./buildImage.sh and ./run-redfish-simulator.sh
#   ## 4. Check that a container is running and listening on port 8000, by command: docker ps
#   ## 5. Now run Telegraf with redfish plugin.
#
#   ## Server url
#   address = "http://localhost:8000/redfish/v1"
#
#   ## Username, Password for hardware server
#   username = "root"
#   password = "password123456"
#
#   ## ComputerSystemId
#   computer_system_id="1"
#
#   ## Amount of time allowed to complete the HTTP request
#   # timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false


# Read metrics from storage devices supporting S.M.A.R.T.
[[inputs.smart]]
  ## Optionally specify the path to the smartctl executable
  # path_smartctl = "/usr/bin/smartctl"

  ## Optionally specify the path to the nvme-cli executable
  # path_nvme = "/usr/bin/nvme"

  ## Optionally specify if vendor specific attributes should be propagated for NVMe disk case
  ## ["auto-on"] - automatically find and enable additional vendor specific disk info
  ## ["vendor1", "vendor2", ...] - e.g. "Intel" enable additional Intel specific disk info
  # enable_extensions = ["auto-on"]

  ## On most platforms used cli utilities requires root access.
  ## Setting 'use_sudo' to true will make use of sudo to run smartctl or nvme-cli.
  ## Sudo must be configured to allow the telegraf user to run smartctl or nvme-cli
  ## without a password.
  use_sudo = true

  ## Skip checking disks in this power mode. Defaults to
  ## "standby" to not wake up disks that have stopped rotating.
  ## See --nocheck in the man pages for smartctl.
  ## smartctl version 5.41 and 5.42 have faulty detection of
  ## power mode and might require changing this value to
  ## "never" depending on your disks.
  # nocheck = "standby"

  ## Gather all returned S.M.A.R.T. attribute metrics and the detailed
  ## information from each drive into the 'smart_attribute' measurement.
  attributes = true

  ## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.
  # excludes = [ "/dev/pass6" ]

  ## Optionally specify devices and device type, if unset
  ## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done
  ## and all found will be included except for the excluded in excludes.
  # devices = [ "/dev/ada0 -d atacam", "/dev/nvme0"]

  ## Timeout for the cli command to complete.
  # timeout = "30s"

  ## Optionally call smartctl and nvme-cli with a specific concurrency policy.
  ## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.
  ## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of
  ## SMART data - one individual array drive at the time. In such case please set this configuration option
  ## to "sequential" to get readings for all drives.
  ## valid options: concurrent, sequential
  # read_method = "concurrent"


# Read metrics about system load & uptime
[[inputs.system]]
  ## Uncomment to remove deprecated metrics.
  # fielddrop = ["uptime_format"]


# Read metrics about temperature
[[inputs.temp]]
  # no configuration


###############################################################################
#                            SERVICE INPUT PLUGINS                            #
###############################################################################


# # Intel Performance Monitoring Unit plugin exposes Intel PMU metrics available through Linux Perf subsystem
# [[inputs.intel_pmu]]
#   ## List of filesystem locations of JSON files that contain PMU event definitions.
#   event_definitions = ["/var/cache/pmu/GenuineIntel-6-55-4-core.json", "/var/cache/pmu/GenuineIntel-6-55-4-uncore.json"]
#
#   ## List of core events measurement entities. There can be more than one core_events sections.
#   [[inputs.intel_pmu.core_events]]
#     ## List of events to be counted. Event names shall match names from event_definitions files.
#     ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.
#     ## If absent, all core events from provided event_definitions are counted skipping unresolvable ones.
#     events = ["INST_RETIRED.ANY", "CPU_CLK_UNHALTED.THREAD_ANY:config1=0x4043200000000k"]
#
#     ## Limits the counting of events to core numbers specified.
#     ## If absent, events are counted on all cores.
#     ## Single "0", multiple "0,1,2" and range "0-2" notation is supported for each array element.
#     ##   example: cores = ["0,2", "4", "12-16"]
#     cores = ["0"]
#
#     ## Indicator that plugin shall attempt to run core_events.events as a single perf group.
#     ## If absent or set to false, each event is counted individually. Defaults to false.
#     ## This limits the number of events that can be measured to a maximum of available hardware counters per core.
#     ## Could vary depending on type of event, use of fixed counters.
#     # perf_group = false
#
#     ## Optionally set a custom tag value that will be added to every measurement within this events group.
#     ## Can be applied to any group of events, unrelated to perf_group setting.
#     # events_tag = ""
#
#   ## List of uncore event measurement entities. There can be more than one uncore_events sections.
#   [[inputs.intel_pmu.uncore_events]]
#     ## List of events to be counted. Event names shall match names from event_definitions files.
#     ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.
#     ## If absent, all uncore events from provided event_definitions are counted skipping unresolvable ones.
#     events = ["UNC_CHA_CLOCKTICKS", "UNC_CHA_TOR_OCCUPANCY.IA_MISS"]
#
#     ## Limits the counting of events to specified sockets.
#     ## If absent, events are counted on all sockets.
#     ## Single "0", multiple "0,1" and range "0-1" notation is supported for each array element.
#     ##   example: sockets = ["0-2"]
#     sockets = ["0"]
#
#     ## Indicator that plugin shall provide an aggregated value for multiple units of same type distributed in an uncore.
#     ## If absent or set to false, events for each unit are exposed as separate metric. Defaults to false.
#     # aggregate_uncore_units = false
#
#     ## Optionally set a custom tag value that will be added to every measurement within this events group.
#     # events_tag = ""



# # Intel Resource Director Technology plugin
# [[inputs.intel_rdt]]
# 	## Optionally set sampling interval to Nx100ms.
# 	## This value is propagated to pqos tool. Interval format is defined by pqos itself.
# 	## If not provided or provided 0, will be set to 10 = 10x100ms = 1s.
# 	# sampling_interval = "10"
#
# 	## Optionally specify the path to pqos executable.
# 	## If not provided, auto discovery will be performed.
# 	# pqos_path = "/usr/local/bin/pqos"
#
# 	## Optionally specify if IPC and LLC_Misses metrics shouldn't be propagated.
# 	## If not provided, default value is false.
# 	# shortened_metrics = false
#
# 	## Specify the list of groups of CPU core(s) to be provided as pqos input.
# 	## Mandatory if processes aren't set and forbidden if processes are specified.
# 	## e.g. ["0-3", "4,5,6"] or ["1-3,4"]
# 	cores = ["0-3"]
#
# 	## Specify the list of processes for which Metrics will be collected.
# 	## Mandatory if cores aren't set and forbidden if cores are specified.
# 	## e.g. ["qemu", "pmd"]
# 	# processes = ["process"]
#
# 	## Specify if the pqos process should be called with sudo.
# 	## Mandatory if the telegraf process does not run as root.
# 	# use_sudo = false


# # RAS plugin exposes counter metrics for Machine Check Errors provided by RASDaemon (sqlite3 output is required).
# [[inputs.ras]]
#   ## Optional path to RASDaemon sqlite3 database.
#   ## Default: /var/lib/rasdaemon/ras-mc_event.db
#   # db_path = ""

